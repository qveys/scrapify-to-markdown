import os
import requests
import sys
import time
import threading
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from markdownify import markdownify as md

def is_same_domain(base_url, test_url):
    """VÃ©rifie si les deux URLs appartiennent au mÃªme domaine."""
    base_domain = urlparse(base_url).netloc
    test_domain = urlparse(test_url).netloc
    return base_domain == test_domain

def get_all_links(url, base_url, stats):
    """RÃ©cupÃ¨re tous les liens de la page qui appartiennent au mÃªme domaine."""
    try:
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        links = set()
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            full_url = urljoin(url, href)
            # Ignorer les liens avec des query parameters ou des ancres
            if '?' in full_url or '#' in full_url:
                continue
            
            # Normaliser l'URL en supprimant le "/" final (sauf pour la racine)
            if full_url.endswith('/') and len(urlparse(full_url).path) > 1:
                full_url = full_url[:-1]
            
            if is_same_domain(base_url, full_url) and not full_url.endswith(('.pdf', '.jpg', '.png', '.gif')):
                links.add(full_url)
        
        stats['links_found'] += len(links)
        return links
    except Exception as e:
        stats['errors'] += 1
        return set()

def save_as_markdown(url, output_dir, stats):
    """Sauvegarde le contenu de l'URL en Markdown."""
    try:
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        html = response.text
        markdown = md(html)
        parsed_url = urlparse(url)
        
        # CrÃ©er un nom de fichier basÃ© sur le chemin de l'URL
        path = parsed_url.path.strip('/').replace('/', '_') or 'index'
        base_filename = f"{output_dir}/{path}.md"
        
        # CrÃ©er le dossier parent si nÃ©cessaire
        os.makedirs(os.path.dirname(base_filename), exist_ok=True)
        
        # VÃ©rifier si le fichier existe et crÃ©er un nom unique si nÃ©cessaire
        filename = base_filename
        counter = 1
        while os.path.exists(filename):
            name_part = base_filename[:-3]  # Enlever .md
            filename = f"{name_part}_{counter}.md"
            counter += 1
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# {url}\n\n{markdown}")
        
        stats['pages_converted'] += 1
        return True
    except Exception as e:
        stats['errors'] += 1
        return False

def clear_screen():
    """Efface l'Ã©cran et remet le curseur en haut."""
    os.system('clear' if os.name == 'posix' else 'cls')

def display_progress(current_url, links_found, stats, start_time):
    """Affiche le progrÃ¨s dans un format structurÃ©."""
    # Effacer l'Ã©cran
    clear_screen()
    
    # Affichage de la page en cours
    print(f"ğŸ” Page en cours d'analyse : {current_url}")
    print(f"ğŸŒ€ {links_found} liens trouvÃ©s sur cette page")
    print()
    
    # Tableau de progression
    print("|-----------------------|")
    print("|     ğŸš€ Progression    |")
    print("|-----------------------|")
    print(f"| Converties |   {stats['pages_converted']:6d} |")
    print(f"| Restantes  |   {stats['remaining']:6d} |")
    print(f"| VisitÃ©es   |   {stats['already_visited']:6d} |")
    print(f"| ErronÃ©es   |   {stats['errors']:6d} |")
    print(f"| Parcourus  |   {stats['links_found']:6d} |")
    print("|-----------------------|")
    print()
    print()
    
    # Temps Ã©coulÃ©
    elapsed_time = time.time() - start_time
    minutes = int(elapsed_time // 60)
    seconds = int(elapsed_time % 60)
    print(f"â±ï¸  Temps Ã©coulÃ© : {minutes:02d}:{seconds:02d}")
    print()

def crawl(url, base_url, output_dir, start_time):
    """Parcourt itÃ©rativement les liens du mÃªme domaine."""
    visited = set()
    queue = [url]  # Queue des URLs Ã  traiter
    stats = {'pages_converted': 0, 'remaining': 0, 'links_found': 0, 'errors': 0, 'already_visited': 0}
    
    while queue:
        current_url = queue.pop(0)  # Prendre la premiÃ¨re URL de la queue
        
        # Marquer comme visitÃ©e AVANT de vÃ©rifier
        visited.add(current_url)
        
        # Traiter la page
        save_as_markdown(current_url, output_dir, stats)
        links = get_all_links(current_url, base_url, stats)
        
        # Ajouter les nouveaux liens Ã  la queue (en Ã©vitant les doublons)
        new_links = []
        for link in links:
            if link not in visited and link not in queue:
                new_links.append(link)
            elif link in visited:
                stats['already_visited'] += 1
        
        queue.extend(new_links)
        
        # Ordonner la queue par ordre alphabÃ©tique
        queue.sort()
        
        # Mise Ã  jour des statistiques
        stats['remaining'] = len(queue)
        
        # Affichage du progrÃ¨s
        display_progress(current_url, len(links), stats, start_time)
        time.sleep(0.5)  # Pause pour voir l'affichage
    
    return stats

if __name__ == "__main__":
    start_url = input("Entrez l'URL de dÃ©part: ").strip()
    
    # CrÃ©er un dossier parent basÃ© sur l'URL de base
    parsed_start_url = urlparse(start_url)
    domain_name = parsed_start_url.netloc.replace('www.', '').replace('.', '_')
    output_directory = f"markdown_pages_{domain_name}"
    os.makedirs(output_directory, exist_ok=True)
    
    print(f"ğŸ¯ DÃ©marrage du crawling de {start_url}")
    print(f"ğŸ“ Dossier de sortie: {output_directory}/")
    print("=" * 60)
    input("Appuyez sur EntrÃ©e pour commencer...")
    
    start_time = time.time()
    stats = crawl(start_url, start_url, output_directory, start_time)
    
    end_time = time.time()
    clear_screen()
    print("=" * 60)
    print(f"ğŸ‰ Crawling terminÃ© en {end_time - start_time:.2f} secondes!")
    print("=" * 60)
    print(f"ğŸ“Š Statistiques finales:")
    print(f"   â€¢ Pages converties: {stats['pages_converted']}")
    print(f"   â€¢ Liens trouvÃ©s: {stats['links_found']}")
    print(f"   â€¢ Pages dÃ©jÃ  visitÃ©es: {stats['already_visited']}")
    print(f"   â€¢ Erreurs: {stats['errors']}")
    print(f"ğŸ“ Fichiers sauvegardÃ©s dans: {output_directory}/")
